# PROJECT_DISPLAY_NAME

PROJECT_DESCRIPTION

## Features

- **Production Ready**: Kubernetes health checks, structured logging, and graceful shutdown
- **OpenAPI Documentation**: Comprehensive API specification with interactive docs
- **Observability**: Health monitoring, structured logging with dynamic level control
- **Container Ready**: Multi-stage Docker builds and Docker Compose development environment
- **Release Management**: Automated versioning and releases with GoReleaser
{{#USE_POSTGRES}}
- **PostgreSQL Integration**: Connection pooling, health checks, and configuration management
{{/USE_POSTGRES}}
{{#USE_KAFKA}}
- **Kafka Messaging**: Producer/consumer patterns with SASL/SSL support and delivery guarantees
{{#USE_SCHEMA_REGISTRY}}
- **Schema Registry**: Avro serialization with automatic schema evolution and compatibility
{{/USE_SCHEMA_REGISTRY}}
{{/USE_KAFKA}}

## Project Structure

```
.
├── cmd/
│   └── server/
│       └── main.go          # Application entry point
├── internal/
│   ├── api/                 # HTTP handlers and routing
│   ├── config/              # Configuration management
{{#USE_POSTGRES}}│   ├── db/                  # Database connection and operations{{/USE_POSTGRES}}
│   ├── health/              # Health check implementation
{{#USE_KAFKA}}│   ├── kafka/               # Kafka client implementation{{/USE_KAFKA}}
│   ├── logger/              # Structured logging setup
│   └── version/             # Version information
├── api/
│   ├── openapi.yaml         # Generated OpenAPI specification
│   ├── openapi.json         # Generated OpenAPI specification (JSON)
│   └── openapi/             # OpenAPI specification sources
│       ├── base.yaml        # Base specification with schemas and info
│       ├── standard.yaml    # Standard endpoints (health, version, admin)
│       └── application.yaml # Application-specific endpoints
├── k8s/                     # Kubernetes manifests
├── test/                    # Additional test files
├── Dockerfile               # Multi-stage Docker build
├── Makefile                 # Build and development tasks
├── go.mod                   # Go module definition
└── README.md                # This file
```

## Quick Start

### Prerequisites

- **Go 1.24+**: Required for building and running the application
- **Docker**: For containerized development and deployment
- **Make**: For running development tasks
{{#USE_POSTGRES}}
- **PostgreSQL**: Database server for local development
{{/USE_POSTGRES}}
{{#USE_KAFKA}}
- **Apache Kafka**: Message broker for event streaming
{{#USE_SCHEMA_REGISTRY}}
- **Confluent Schema Registry**: Schema management for Avro serialization
{{/USE_SCHEMA_REGISTRY}}
{{/USE_KAFKA}}

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd PROJECT_NAME
```

2. Install Go dependencies:
```bash
make deps
```

3. Set up your environment:
```bash
# Copy example environment file
cp .env.example .env
# Edit .env with your configuration
```

4. Build the application:
```bash
make build
```

## Running the Service

### Local Development

Start the required services:
```bash
make dev-env
```

Run the application:
```bash
make run-dev
```

The service will be available at `http://localhost:8080`.

### Using Docker

Build and run with Docker:
```bash
make docker-build
make docker-run
```

### Using Docker Compose

Run the entire stack:
```bash
docker-compose up
```

This starts:
- The application on port 8080
{{#USE_POSTGRES}}
- PostgreSQL on port 5432
{{/USE_POSTGRES}}
{{#USE_KAFKA}}
- Kafka on port 9092
- Zookeeper on port 2181
{{#USE_SCHEMA_REGISTRY}}
- Schema Registry on port 8081
{{/USE_SCHEMA_REGISTRY}}
{{/USE_KAFKA}}

## Available Endpoints

### Health Checks
- `GET /health/live` - Liveness probe (always returns 200)
- `GET /health/ready` - Readiness probe (checks dependencies)

### Information
- `GET /version` - Get build version information
- `GET /api/v1/admin/log-level` - Get current log level
- `PUT /api/v1/admin/log-level` - Change log level dynamically

### API Examples
- `GET /api/v1/hello` - Simple hello endpoint
- `POST /api/v1/echo` - Echo request body

### Documentation
- `GET /openapi.yaml` - OpenAPI 3.0 specification (YAML)
- `GET /openapi.json` - OpenAPI 3.0 specification (JSON)

## Usage Examples

### Changing Log Level Remotely

Get current log level:
```bash
curl http://localhost:8080/api/v1/admin/log-level
```

Change to debug level:
```bash
curl -X PUT http://localhost:8080/api/v1/admin/log-level \
  -H "Content-Type: application/json" \
  -d '{"level": "debug"}'
```

### Basic API Usage

Hello endpoint:
```bash
curl http://localhost:8080/api/v1/hello
```

Echo endpoint:
```bash
curl -X POST http://localhost:8080/api/v1/echo \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello World", "timestamp": "2024-01-01T00:00:00Z"}'
```

{{#USE_POSTGRES}}
## Database

The service connects to PostgreSQL using the following environment variables:

- `DB_HOST` - Database host (default: localhost)
- `DB_PORT` - Database port (default: 5432)
- `DB_USER` - Database user (default: postgres)
- `DB_PASSWORD` - Database password
- `DB_NAME` - Database name (default: gobase)
- `DB_SSLMODE` - SSL mode (default: disable)
- `DB_MAX_OPEN_CONNS` - Maximum open connections (default: 25)
- `DB_MAX_IDLE_CONNS` - Maximum idle connections (default: 5)
- `DB_CONN_MAX_LIFETIME` - Connection lifetime in minutes (default: 5)

### Database Usage Example

The database connection is available through the health check and can be extended for your application needs.

{{/USE_POSTGRES}}
{{#USE_KAFKA}}
## Message Streaming with Kafka

This service integrates with Apache Kafka for reliable message streaming and event-driven architecture.

### Architecture

The Kafka integration provides:
- **Producer Pattern**: Async message publishing with delivery guarantees
- **Consumer Pattern**: Reliable message consumption with offset management
- **Error Handling**: Dead letter queues and retry mechanisms
- **Monitoring**: Producer/consumer metrics and health checks

### Configuration

Configure Kafka connection via environment variables:

```bash
# Broker Configuration
KAFKA_BROKERS=localhost:9092              # Comma-separated broker list
KAFKA_TOPIC=events                        # Default topic name
KAFKA_GROUP_ID=PROJECT_NAME               # Consumer group ID

# Security Configuration
KAFKA_SECURITY_PROTOCOL=PLAINTEXT        # PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL
KAFKA_SASL_MECHANISM=PLAIN               # PLAIN, SCRAM-SHA-256, SCRAM-SHA-512
KAFKA_SASL_USERNAME=your-username        # SASL username
KAFKA_SASL_PASSWORD=your-password        # SASL password
```

{{#USE_SCHEMA_REGISTRY}}
### Schema Registry Integration

Avro serialization with automatic schema evolution:

```bash
# Schema Registry Configuration
SCHEMA_REGISTRY_URL=http://localhost:8081 # Registry endpoint
SCHEMA_REGISTRY_USERNAME=registry-user    # Basic auth username (optional)
SCHEMA_REGISTRY_PASSWORD=registry-pass    # Basic auth password (optional)
SCHEMA_REGISTRY_API_KEY=api-key          # API key auth (optional)
SCHEMA_REGISTRY_API_SECRET=api-secret    # API secret auth (optional)
```

#### Schema Management

- **Automatic Evolution**: Schemas evolve automatically with backward/forward compatibility
- **Type Safety**: Strongly typed Go structs generated from Avro schemas  
- **Version Control**: Schema versions managed centrally in Schema Registry
- **Compatibility Checks**: Prevents breaking changes to message formats

#### Usage Example

```go
// Define your Avro schema-compatible struct
type UserEvent struct {
    UserID    string    `avro:"user_id"`
    Action    string    `avro:"action"`
    Timestamp time.Time `avro:"timestamp"`
}

// Producer will automatically serialize to Avro
producer.PublishUserEvent(UserEvent{
    UserID:    "user123",
    Action:    "login",
    Timestamp: time.Now(),
})
```

{{/USE_SCHEMA_REGISTRY}}
### Production Features

- **Idempotent Producers**: Exactly-once delivery semantics
- **Consumer Groups**: Horizontal scaling and load balancing
- **Health Monitoring**: Built-in health checks for broker connectivity
- **Graceful Shutdown**: Clean consumer group leaving and offset commits
- **Error Recovery**: Automatic reconnection and retry logic

{{/USE_KAFKA}}
## API Documentation

The service provides a comprehensive OpenAPI 3.0 specification using a modular approach:

### OpenAPI Structure

- **`api/openapi/base.yaml`** - Base specification with schemas, info, and common components
- **`api/openapi/standard.yaml`** - Standard endpoints (health checks, version, admin)
- **`api/openapi/application.yaml`** - Application-specific business endpoints
- **`api/openapi.yaml`** - Generated merged specification (YAML)
- **`api/openapi.json`** - Generated merged specification (JSON)

### Available Endpoints

- `GET /openapi.yaml` - OpenAPI 3.0 specification (YAML format)
- `GET /openapi.json` - OpenAPI 3.0 specification (JSON format)

### Modifying the API Specification

1. Edit the source files in `api/openapi/`:
   - Add new schemas to `base.yaml`
   - Add standard endpoints to `standard.yaml`
   - Add business endpoints to `application.yaml`

2. Regenerate the merged specification:
```bash
make openapi
```

3. The merged files are automatically included in Docker builds and releases.

## Development

### Running Tests
```bash
make test
```

### Running Tests with Coverage
```bash
make coverage
```

### Code Formatting
```bash
make fmt
```

### Linting
```bash
make lint
```

### Generate OpenAPI Spec
```bash
make openapi
```

## Environment Variables

### Application Settings
- `PORT` - HTTP server port (default: 8080)
- `LOG_LEVEL` - Log level: debug, info, warn, error (default: info)

{{#USE_POSTGRES}}
### Database Settings
- `DB_HOST` - Database host (default: localhost)
- `DB_PORT` - Database port (default: 5432)
- `DB_USER` - Database user (default: postgres)
- `DB_PASSWORD` - Database password
- `DB_NAME` - Database name (default: gobase)
- `DB_SSLMODE` - SSL mode (default: disable)
- `DB_MAX_OPEN_CONNS` - Maximum open connections (default: 25)
- `DB_MAX_IDLE_CONNS` - Maximum idle connections (default: 5)
- `DB_CONN_MAX_LIFETIME` - Connection lifetime in minutes (default: 5)

{{/USE_POSTGRES}}
{{#USE_KAFKA}}
### Kafka Settings
- `KAFKA_BROKERS` - Comma-separated broker list (default: localhost:9092)
- `KAFKA_TOPIC` - Default topic name (default: events)
- `KAFKA_GROUP_ID` - Consumer group ID (default: PROJECT_NAME)
- `KAFKA_SECURITY_PROTOCOL` - Security protocol (default: PLAINTEXT)
- `KAFKA_SASL_MECHANISM` - SASL mechanism
- `KAFKA_SASL_USERNAME` - SASL username
- `KAFKA_SASL_PASSWORD` - SASL password

{{#USE_SCHEMA_REGISTRY}}
### Schema Registry Settings
- `SCHEMA_REGISTRY_URL` - Registry endpoint (default: http://localhost:8081)
- `SCHEMA_REGISTRY_USERNAME` - Basic auth username
- `SCHEMA_REGISTRY_PASSWORD` - Basic auth password
- `SCHEMA_REGISTRY_API_KEY` - API key
- `SCHEMA_REGISTRY_API_SECRET` - API secret

{{/USE_SCHEMA_REGISTRY}}
{{/USE_KAFKA}}
## Deployment

### Kubernetes

Apply the Kubernetes manifests:
```bash
kubectl apply -f k8s/
```

### Docker

The service includes optimized Docker builds:

**Development build:**
```bash
make docker-build
```

**Production build (via GoReleaser):**
```bash
make release-snapshot
```

## Release Management

This project uses [GoReleaser](https://goreleaser.com/) for automated releases:

```bash
# Initialize first release
make release-init

# Create patch release
make release-patch

# Create minor release  
make release-minor

# Create major release
make release-major
```

## Monitoring and Observability

### Health Checks

The service provides Kubernetes-compatible health endpoints:

- **Liveness**: `/health/live` - Always returns 200 if service is running
- **Readiness**: `/health/ready` - Returns 200 only if all dependencies are healthy

### Logging

Structured logging using Go's `slog` package:
- JSON format in production
- Configurable log levels
- Dynamic log level changes via API

### Metrics

The service is designed to be easily extended with metrics collection:
- Ready for Prometheus integration
- Health check status monitoring
- Custom application metrics

## Configuration

All configuration is done via environment variables with sensible defaults for local development.

Create a `.env` file for local development:

```bash
# Application
PORT=8080
LOG_LEVEL=debug

{{#USE_POSTGRES}}
# Database
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASSWORD=postgres
DB_NAME=PROJECT_NAME
DB_SSLMODE=disable

{{/USE_POSTGRES}}
{{#USE_KAFKA}}
# Kafka
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=events
KAFKA_GROUP_ID=PROJECT_NAME
KAFKA_SECURITY_PROTOCOL=PLAINTEXT

{{#USE_SCHEMA_REGISTRY}}
# Schema Registry
SCHEMA_REGISTRY_URL=http://localhost:8081

{{/USE_SCHEMA_REGISTRY}}
{{/USE_KAFKA}}
```
